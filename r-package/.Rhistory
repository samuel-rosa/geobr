ifelse(code_state== 50, "MS",
ifelse(code_state== 51, "MT",
ifelse(code_state== 52, "GO",
ifelse(code_state== 53, "DF",NA))))))))))))))))))))))))))))
# Add Region codes and names
temp_sf <- add_region_info(temp_sf,'code_state')
# reorder columns
temp_sf <- dplyr::select(temp_sf,'code_muni', 'name_muni', 'code_state', 'abbrev_state', 'name_state', 'code_region', 'name_region', 'geom')
# Use UTF-8 encoding
temp_sf <- use_encoding_utf8(temp_sf)
# Capitalize the first letter
temp_sf$name_muni <- stringr::str_to_title(temp_sf$name_muni)
# Harmonize spatial projection CRS, using SIRGAS 2000 epsg (SRID): 4674
temp_sf <- harmonize_projection(temp_sf)
# Make any invalid geom valid # st_is_valid( sf)
temp_sf <- sf::st_make_valid(temp_sf)
# keep code as.numeric()
temp_sf$code_muni <- as.numeric(temp_sf$code_muni)
# simplify
temp_sf_simplified <- simplify_temp_sf(temp_sf)
# convert to MULTIPOLYGON
temp_sf <- to_multipolygon(temp_sf)
temp_sf_simplified <- to_multipolygon(temp_sf_simplified)
# Save cleaned sf in the cleaned directory
# i <- gsub("original", "cleaned", i)
dir.dest.file <- paste0(dir.dest,"/")
file.name <- paste0(unique(temp_sf$code_state),"MU",".gpkg")
i <- paste0(dir.dest.file,file.name)
sf::st_write(temp_sf, i , delete_layer = TRUE)
i <- gsub(".gpkg", "_simplified.gpkg", i)
sf::st_write(temp_sf_simplified, i , delete_layer = TRUE)
}
}
# apply function in parallel
future::plan(multisession)
future_map(sub_dirs, clean_muni)
library(RCurl)
library(dplyr)
library(stringr)
library(sf)
library(magrittr)
library(data.table)
library(parallel)
library(lwgeom)
library(readr)
library(furrr)
library(future)
library(mapview)
mapviewOptions(platform = 'leafgl')
source("./prep_data/prep_functions.R")
# Root directory
root_geobr <- getwd()
root_dir <- "L:////# DIRUR #//ASMEQ//geobr//data-raw"
setwd(root_dir)
# Create Directory to keep original downloaded files
destdir_raw <- "./weighting_area"
dir.create(destdir_raw)
setwd(destdir_raw)
# create directory to save original shape files in sf format
dir.create(file.path("shapes_in_sf_all_years_original"), showWarnings = FALSE)
# create directory to save cleaned shape files in sf format
dir.create(file.path("sf_all_years_cleaned"), showWarnings = FALSE)
# select file shp
raw_shapes <- list.files(full.names = T, pattern = ".shp$")
# list code_state
state <- geobr::read_state()
state <- unique(state$code_state)
original_sf <- st_read(raw_shapes, quiet = T, stringsAsFactors=F, options = "ENCODING=UTF-8")
head(original_sf)
# list all files
original_shapes <- list.files(path="./shapes_in_sf_all_years_original" ,full.names = T, pattern = ".rds")
original_shapes
munis <- geobr::read_municipality(year=2010)
munis$geom <- NULL
munis <- select(munis, code_muni, name_muni)
munis$code_muni <- as.character(munis$code_muni)
f=original_shapes[20]
f
### read data
# temp <- readRDS(f, file = f) #o arquivo abre por aqui, logo n?o est? corrompido
# temp_sf1 <- st_read(f, quiet = F, stringsAsFactors=F, options = "ENCODING=UTF8")
temp_sf1 <- readr::read_rds(f)
temp_sf1
names(temp_sf1) <- names(temp_sf1) %>% tolower()
colnames(temp_sf1)[colnames(temp_sf1) %in% c("cd_aponde","area_pond")] <- "code_weighting"
temp_sf1 <- select(temp_sf1, 'code_weighting', 'geometry')
temp_sf2 <- add_state_info(temp_sf1, 'code_weighting')
temp_sf2 <- add_region_info(temp_sf2, 'code_weighting')
temp_sf2 <- dplyr::mutate(temp_sf2, code_muni = str_sub(code_weighting,1,7))
# add municipality name
temp_sf2 <- left_join(temp_sf2, munis)
###### reorder columns -----------------
temp_sf2 <- select(temp_sf2, code_weighting, code_muni, name_muni, code_state, abbrev_state, code_region, name_region, geometry )
temp_sf3 <- harmonize_projection(temp_sf2)
st_crs(temp_sf3)$epsg
st_crs(temp_sf3)$input
st_crs(temp_sf3)$proj4string
st_crs(st_crs(temp_sf3)$wkt) == st_crs(temp_sf3)
# convert all character columns to UTF-8
temp_sf4 <- use_encoding_utf8(temp_sf3)
# remove Z dimension of spatial data
temp_sf5 <- temp_sf4 %>% st_sf() %>% st_zm( drop = T, what = "ZM")
# Make any invalid geometry valid # st_is_valid( sf)
# temp_sf6 <- lwgeom::st_make_valid(temp_sf5)
temp_sf6 <- st_make_valid(temp_sf5)
# simplify
temp_sf7 <- simplify_temp_sf(temp_sf6, tolerance=100)
###### convert to MULTIPOLYGON -----------------
temp_sf6 <- to_multipolygon(temp_sf6)
temp_sf7 <- to_multipolygon(temp_sf7)
# save original and simplified datasets
i <- as.numeric(gsub("\\D", "", f))
i
# readr::write_rds(temp_sf6, path= paste0("./sf_all_years_cleaned/", i, ".rds"),compress = "gz")
sf::st_write(temp_sf6, dsn= paste0("./sf_all_years_cleaned/", i, ".gpkg"))
sf::st_write(temp_sf7, dsn= paste0("./sf_all_years_cleaned/", i,"_simplified", ".gpkg"))
temp_sf6
cleaning_data_fun <- function(f){ # f=original_shapes[20]
### read data
# temp <- readRDS(f, file = f) #o arquivo abre por aqui, logo n?o est? corrompido
# temp_sf1 <- st_read(f, quiet = F, stringsAsFactors=F, options = "ENCODING=UTF8")
temp_sf1 <- readr::read_rds(f)
###### 2. rename column names -----------------
names(temp_sf1) <- names(temp_sf1) %>% tolower()
colnames(temp_sf1)[colnames(temp_sf1) %in% c("cd_aponde","area_pond")] <- "code_weighting"
temp_sf1 <- select(temp_sf1, 'code_weighting', 'geometry')
temp_sf2 <- add_state_info(temp_sf1, 'code_weighting')
temp_sf2 <- add_region_info(temp_sf2, 'code_weighting')
temp_sf2 <- dplyr::mutate(temp_sf2, code_muni = str_sub(code_weighting,1,7))
# add municipality name
temp_sf2 <- left_join(temp_sf2, munis)
###### reorder columns -----------------
temp_sf2 <- select(temp_sf2, code_weighting, code_muni, name_muni, code_state, abbrev_state, code_region, name_region, geometry )
###### 3. ensure the data uses spatial projection SIRGAS 2000 epsg (SRID): 4674-----------------
temp_sf3 <- harmonize_projection(temp_sf2)
st_crs(temp_sf3)$epsg
st_crs(temp_sf3)$input
st_crs(temp_sf3)$proj4string
st_crs(st_crs(temp_sf3)$wkt) == st_crs(temp_sf3)
###### 4. ensure every string column is as.character with UTF-8 encoding -----------------
# convert all character columns to UTF-8
temp_sf4 <- use_encoding_utf8(temp_sf3)
###### 5. remove Z dimension of spatial data-----------------
# remove Z dimension of spatial data
temp_sf5 <- temp_sf4 %>% st_sf() %>% st_zm( drop = T, what = "ZM")
###### 6. fix eventual topology issues in the data-----------------
# Make any invalid geometry valid # st_is_valid( sf)
# temp_sf6 <- lwgeom::st_make_valid(temp_sf5)
temp_sf6 <- st_make_valid(temp_sf5)
###### 7. generate a lighter version of the dataset with simplified borders -----------------
# skip this step if the dataset is made of points, regular spatial grids or rater data
# simplify
temp_sf7 <- simplify_temp_sf(temp_sf6, tolerance=100)
# mapview(temp_sf7)
###### convert to MULTIPOLYGON -----------------
temp_sf6 <- to_multipolygon(temp_sf6)
temp_sf7 <- to_multipolygon(temp_sf7)
###### 8. Clean data set and save it in geopackage format-----------------
# save original and simplified datasets
i <- as.numeric(gsub("\\D", "", f))
# readr::write_rds(temp_sf6, path= paste0("./sf_all_years_cleaned/", i, ".rds"),compress = "gz")
sf::st_write(temp_sf6, dsn= paste0("./sf_all_years_cleaned/", i, ".gpkg"))
sf::st_write(temp_sf7, dsn= paste0("./sf_all_years_cleaned/", i,"_simplified", ".gpkg"))
}
pbapply::pblapply(X=original_shapes, FUN = cleaning_data_fun)
pbapply::pblapply(X=original_shapes, FUN = cleaning_data_fun)
source("./prep_data/prep_functions.R")
source("./prep_data/amc_algorithm/_Crosswalk_main.R")
##### list o all years available  ------------------------------
years_available <- c(1872,1900,1911,1920,1933,1940,1950,1960,1970,1980,1991,2000,2010)
# get combinations
all_combinations <- expand.grid(years_available, years_available) %>% as.data.frame()
names(all_combinations) <- c('start_year', 'end_year')
all_combinations <- subset(all_combinations, start_year < end_year)
prep_amc <- function(i){
table_amc(startyear = all_combinations$start_year[i],
endyear = all_combinations$end_year[i]
)
}
# apply function in parallel
future::plan("multisession")
furrr::future_map(.x = c(5, 10, 20, 30, 40, 60, 70),
.f = prep_amc,
.progress = TRUE
)
2+2
all_combinations
# apply function in parallel
future::plan("multisession")
####### Load Support functions to use in the preprocessing of the data  ------------------------------
source("./prep_data/prep_functions.R")
source("./prep_data/amc_algorithm/_Crosswalk_main.R")
##### list o all years available  ------------------------------
years_available <- c(1872,1900,1911,1920,1933,1940,1950,1960,1970,1980,1991,2000,2010)
# get combinations
all_combinations <- expand.grid(years_available, years_available) %>% as.data.frame()
names(all_combinations) <- c('start_year', 'end_year')
all_combinations <- subset(all_combinations, start_year < end_year)
####### Function prep AMC ------------------------------
prep_amc <- function(i){
table_amc(startyear = all_combinations$start_year[i],
endyear = all_combinations$end_year[i]
)
}
# apply function in parallel
future::plan("multisession")
furrr::future_map(.x = 1:nrow(all_combinations),
.f = prep_amc,
.progress = TRUE
)
2+2
5+5
# create empty metadata
metadata <- data.frame(matrix(ncol = 5, nrow = 0))
colnames(metadata) <- c("geo","year","code","download_path","code_abrev")
# list all data files available in the geobr package
geo=list.files("//storage1/geobr/data_gpkg")
# populate the metadata table
for (a in geo) {    # a="setor_censitario"
ano=list.files(paste("//storage1/geobr/data_gpkg",a,sep="/"))
for (b in ano) { # b=2000
estado=list.files(paste("//storage1/geobr/data_gpkg",a,b,sep="/"))
for (c in estado) { #c="Urbano"
if (c=="Urbano"|c=="Rural"){
estado2=list.files(paste("//storage1/geobr/data_gpkg",a,b,c,sep="/"))
for (d in estado2) { #d=estado2[1]
if (c=="Urbano") {
metadata[nrow(metadata) + 1,] = list(a,b,paste0("U",substr(d, 1, 2)),paste("http://www.ipea.gov.br/geobr/data_gpkg",a,b,c,d,sep="/"))
}
if (c=="Rural") {
metadata[nrow(metadata) + 1,] = list(a,b,paste0("R",substr(d, 1, 2)),paste("http://www.ipea.gov.br/geobr/data_gpkg",a,b,c,d,sep="/"))
}
}
} else {
metadata[nrow(metadata) + 1,] = list(a,b,substr(c, 1, 2),paste("http://www.ipea.gov.br/geobr/data_gpkg",a,b,c,sep="/"))}
}
}
}
# get code abbreviations
library(data.table)
setDT(metadata)
metadata[ grepl("11", substr(code, 1, 3)), code_abrev :=	"RO" ]
metadata[ grepl("12", substr(code, 1, 3)), code_abrev :=	"AC" ]
metadata[ grepl("13", substr(code, 1, 3)), code_abrev :=	"AM" ]
metadata[ grepl("14", substr(code, 1, 3)), code_abrev :=	"RR" ]
metadata[ grepl("15", substr(code, 1, 3)), code_abrev :=	"PA" ]
metadata[ grepl("16", substr(code, 1, 3)), code_abrev :=	"AP" ]
metadata[ grepl("17", substr(code, 1, 3)), code_abrev :=	"TO" ]
metadata[ grepl("21", substr(code, 1, 3)), code_abrev :=	"MA" ]
metadata[ grepl("22", substr(code, 1, 3)), code_abrev :=	"PI" ]
metadata[ grepl("23", substr(code, 1, 3)), code_abrev :=	"CE" ]
metadata[ grepl("24", substr(code, 1, 3)), code_abrev :=	"RN" ]
metadata[ grepl("25", substr(code, 1, 3)), code_abrev :=	"PB" ]
metadata[ grepl("26", substr(code, 1, 3)), code_abrev :=	"PE" ]
metadata[ grepl("27", substr(code, 1, 3)), code_abrev :=	"AL" ]
metadata[ grepl("28", substr(code, 1, 3)), code_abrev :=	"SE" ]
metadata[ grepl("29", substr(code, 1, 3)), code_abrev :=	"BA" ]
metadata[ grepl("31", substr(code, 1, 3)), code_abrev :=	"MG" ]
metadata[ grepl("32", substr(code, 1, 3)), code_abrev :=	"ES" ]
metadata[ grepl("33", substr(code, 1, 3)), code_abrev :=	"RJ" ]
metadata[ grepl("35", substr(code, 1, 3)), code_abrev :=	"SP" ]
metadata[ grepl("41", substr(code, 1, 3)), code_abrev :=	"PR" ]
metadata[ grepl("42", substr(code, 1, 3)), code_abrev :=	"SC" ]
metadata[ grepl("43", substr(code, 1, 3)), code_abrev :=	"RS" ]
metadata[ grepl("50", substr(code, 1, 3)), code_abrev :=	"MS" ]
metadata[ grepl("51", substr(code, 1, 3)), code_abrev :=	"MT" ]
metadata[ grepl("52", substr(code, 1, 3)), code_abrev :=	"GO" ]
metadata[ grepl("53", substr(code, 1, 3)), code_abrev :=	"DF" ]
# to avoid conflict with data.table
metadata <- as.data.frame(metadata)
table(metadata$geo)
table(metadata$year)
subset(metadata, geo == 'amc')
library(geobr)
start_year=1970
end_year=2010
simplified=TRUE
showProgress=TRUE
# tests
years_available <- c(1872,1900,1911,1920,1933,1940,1950,1960,1970,1980,1991,2000,2010)
if( !(start_year %in% years_available) ){  stop(paste0("Invalid 'start_year'. It must be one of the following: ",
paste(years_available, collapse = " "))) }
if( !(end_year %in% years_available) ){  stop(paste0("Invalid 'end_year'. It must be one of the following: ",
paste(years_available, collapse = " "))) }
if( end_year <= start_year){  stop(paste0("start_year must be smaller than end_year")) }
# Get metadata with data url addresses
temp_meta <- select_metadata(geography="amc_muni", year=start_year, simplified=simplified)
metadata <- download_metadata()
geography='amc'
temp_meta <- subset(metadata, geo == geography)
temp_meta
metadata <- download_metadata()
geography
subset(metadata, geo == geography)
table(metadata$geo)
save updated metadata table
readr::write_csv(metadata,"//storage1/geobr/metadata/metadata_gpkg.csv")
source("./prep_data/prep_functions.R")
source("./prep_data/amc_algorithm/_Crosswalk_main.R")
##### list o all years available  ------------------------------
years_available <- c(1872,1900,1911,1920,1933,1940,1950,1960,1970,1980,1991,2000,2010)
# get combinations
all_combinations <- expand.grid(years_available, years_available) %>% as.data.frame()
names(all_combinations) <- c('start_year', 'end_year')
all_combinations <- subset(all_combinations, start_year < end_year)
prep_amc <- function(i){
table_amc(startyear = all_combinations$start_year[i],
endyear = all_combinations$end_year[i]
)
}
source("./prep_data/prep_functions.R")
source("./prep_data/amc_algorithm/_Crosswalk_main.R")
##### list o all years available  ------------------------------
years_available <- c(1872,1900,1911,1920,1933,1940,1950,1960,1970,1980,1991,2000,2010)
# get combinations
all_combinations <- expand.grid(years_available, years_available) %>% as.data.frame()
names(all_combinations) <- c('start_year', 'end_year')
all_combinations <- subset(all_combinations, start_year < end_year)
prep_amc <- function(i){
table_amc(startyear = all_combinations$start_year[i],
endyear = all_combinations$end_year[i]
)
}
# apply function in parallel
future::plan("multisession")
furrr::future_map(.x = 1:15,
.f = prep_amc,
.progress = TRUE
)
furrr::future_map(.x = 1:nrow(all_combinations),
.f = prep_amc,
.progress = TRUE
)
furrr::future_map(.x = 1:nrow(all_combinations),
.f = prep_amc,
.progress = TRUE
)
####### Load Support functions to use in the preprocessing of the data  ------------------------------
source("./prep_data/prep_functions.R")
source("./prep_data/amc_algorithm/_Crosswalk_main.R")
##### list o all years available  ------------------------------
years_available <- c(1872,1900,1911,1920,1933,1940,1950,1960,1970,1980,1991,2000,2010)
# get combinations
all_combinations <- expand.grid(years_available, years_available) %>% as.data.frame()
names(all_combinations) <- c('start_year', 'end_year')
all_combinations <- subset(all_combinations, start_year < end_year)
####### Function prep AMC ------------------------------
prep_amc <- function(i){
table_amc(startyear = all_combinations$start_year[i],
endyear = all_combinations$end_year[i]
)
}
# apply function in parallel
future::plan("multisession")
furrr::future_map(.x = 1:nrow(all_combinations),
.f = prep_amc,
.progress = TRUE
)
source("./prep_data/prep_functions.R")
source("./prep_data/amc_algorithm/_Crosswalk_main.R")
# get combinations
all_combinations <- expand.grid(years_available, years_available) %>% as.data.frame()
names(all_combinations) <- c('start_year', 'end_year')
all_combinations <- subset(all_combinations, start_year < end_year)
##### list o all years available  ------------------------------
years_available <- c(1872,1900,1911,1920,1933,1940,1950,1960,1970,1980,1991,2000,2010)
prep_amc <- function(i){
table_amc(startyear = all_combinations$start_year[i],
endyear = all_combinations$end_year[i]
)
}
prep_amc <- function(i){
table_amc(startyear = all_combinations$start_year[i],
endyear = all_combinations$end_year[i]
)
}
# apply function in parallel
future::plan("multisession")
furrr::future_map(.x = 1:nrow(all_combinations),
.f = prep_amc,
.progress = TRUE
)
####### Load Support functions to use in the preprocessing of the data  ------------------------------
source("./prep_data/prep_functions.R")
source("./prep_data/amc_algorithm/_Crosswalk_main.R")
##### list o all years available  ------------------------------
years_available <- c(1872,1900,1911,1920,1933,1940,1950,1960,1970,1980,1991,2000,2010)
# get combinations
all_combinations <- expand.grid(years_available, years_available) %>% as.data.frame()
names(all_combinations) <- c('start_year', 'end_year')
all_combinations <- subset(all_combinations, start_year < end_year)
####### Function prep AMC ------------------------------
prep_amc <- function(i){
table_amc(startyear = all_combinations$start_year[i],
endyear = all_combinations$end_year[i]
)
}
# apply function in parallel
future::plan("multisession")
furrr::future_map(.x = 1:nrow(all_combinations),
.f = prep_amc,
.progress = TRUE
)
2+2
# create empty metadata
metadata <- data.frame(matrix(ncol = 5, nrow = 0))
colnames(metadata) <- c("geo","year","code","download_path","code_abrev")
# list all data files available in the geobr package
geo=list.files("//storage1/geobr/data_gpkg")
# populate the metadata table
for (a in geo) {    # a="setor_censitario"
ano=list.files(paste("//storage1/geobr/data_gpkg",a,sep="/"))
for (b in ano) { # b=2000
estado=list.files(paste("//storage1/geobr/data_gpkg",a,b,sep="/"))
for (c in estado) { #c="Urbano"
if (c=="Urbano"|c=="Rural"){
estado2=list.files(paste("//storage1/geobr/data_gpkg",a,b,c,sep="/"))
for (d in estado2) { #d=estado2[1]
if (c=="Urbano") {
metadata[nrow(metadata) + 1,] = list(a,b,paste0("U",substr(d, 1, 2)),paste("http://www.ipea.gov.br/geobr/data_gpkg",a,b,c,d,sep="/"))
}
if (c=="Rural") {
metadata[nrow(metadata) + 1,] = list(a,b,paste0("R",substr(d, 1, 2)),paste("http://www.ipea.gov.br/geobr/data_gpkg",a,b,c,d,sep="/"))
}
}
} else {
metadata[nrow(metadata) + 1,] = list(a,b,substr(c, 1, 2),paste("http://www.ipea.gov.br/geobr/data_gpkg",a,b,c,sep="/"))}
}
}
}
setDT(metadata)
metadata[ grepl("11", substr(code, 1, 3)), code_abrev :=	"RO" ]
metadata[ grepl("12", substr(code, 1, 3)), code_abrev :=	"AC" ]
metadata[ grepl("13", substr(code, 1, 3)), code_abrev :=	"AM" ]
metadata[ grepl("14", substr(code, 1, 3)), code_abrev :=	"RR" ]
metadata[ grepl("15", substr(code, 1, 3)), code_abrev :=	"PA" ]
metadata[ grepl("16", substr(code, 1, 3)), code_abrev :=	"AP" ]
metadata[ grepl("17", substr(code, 1, 3)), code_abrev :=	"TO" ]
metadata[ grepl("21", substr(code, 1, 3)), code_abrev :=	"MA" ]
# get code abbreviations
library(data.table)
metadata[ grepl("22", substr(code, 1, 3)), code_abrev :=	"PI" ]
metadata[ grepl("23", substr(code, 1, 3)), code_abrev :=	"CE" ]
metadata[ grepl("24", substr(code, 1, 3)), code_abrev :=	"RN" ]
metadata[ grepl("25", substr(code, 1, 3)), code_abrev :=	"PB" ]
metadata[ grepl("26", substr(code, 1, 3)), code_abrev :=	"PE" ]
metadata[ grepl("27", substr(code, 1, 3)), code_abrev :=	"AL" ]
metadata[ grepl("28", substr(code, 1, 3)), code_abrev :=	"SE" ]
metadata[ grepl("29", substr(code, 1, 3)), code_abrev :=	"BA" ]
metadata[ grepl("31", substr(code, 1, 3)), code_abrev :=	"MG" ]
metadata[ grepl("32", substr(code, 1, 3)), code_abrev :=	"ES" ]
metadata[ grepl("33", substr(code, 1, 3)), code_abrev :=	"RJ" ]
metadata[ grepl("35", substr(code, 1, 3)), code_abrev :=	"SP" ]
metadata[ grepl("41", substr(code, 1, 3)), code_abrev :=	"PR" ]
metadata[ grepl("42", substr(code, 1, 3)), code_abrev :=	"SC" ]
metadata[ grepl("43", substr(code, 1, 3)), code_abrev :=	"RS" ]
metadata[ grepl("50", substr(code, 1, 3)), code_abrev :=	"MS" ]
metadata[ grepl("51", substr(code, 1, 3)), code_abrev :=	"MT" ]
metadata[ grepl("52", substr(code, 1, 3)), code_abrev :=	"GO" ]
metadata[ grepl("53", substr(code, 1, 3)), code_abrev :=	"DF" ]
# to avoid conflict with data.table
metadata <- as.data.frame(metadata)
table(metadata$geo)
table(metadata$year)
subset(metadata, geo == 'amc')
readr::write_csv(metadata,"//storage1/geobr/metadata/metadata_gpkg.csv")
library(geobr)
start_year=1970
end_year=2010
# tests
years_available <- c(1872,1900,1911,1920,1933,1940,1950,1960,1970,1980,1991,2000,2010)
if( !(start_year %in% years_available) ){  stop(paste0("Invalid 'start_year'. It must be one of the following: ",
paste(years_available, collapse = " "))) }
if( !(end_year %in% years_available) ){  stop(paste0("Invalid 'end_year'. It must be one of the following: ",
paste(years_available, collapse = " "))) }
if( end_year <= start_year){  stop(paste0("start_year must be smaller than end_year")) }
# Get metadata with data url addresses
temp_meta <- select_metadata(geography="amc", year=start_year, simplified=simplified)
simplified=T
geography="amc"
metadata <- download_metadata()
temp_meta <- subset(metadata, geo == geography)
temp_meta <- select_year_input(temp_meta, y = year)
year=start_year
temp_meta <- select_year_input(temp_meta, y = year)
temp_meta
temp_meta <- select_data_type(temp_meta, simplified = simplified)
temp_meta
head(temp_meta)
# list paths of files to download
file_url <- as.character(temp_meta$download_path)
# subset based on end_year
target_year <- paste0(start_year, '-', end_year)
file_url <- file_url[ file_url %like% target_year]
file_url
target_year
file_url
# list paths of files to download
file_url <- as.character(temp_meta$download_path)
file_url
# subset based on end_year
target_year <- paste0(start_year, '_', end_year)
target_year
file_url[ file_url %like% target_year]
file_url <- file_url[ file_url %like% target_year]
# download files
temp_sf <- download_gpkg(file_url, progress_bar = showProgress)
showProgress=T
temp_sf <- download_gpkg(file_url, progress_bar = showProgress)
head(temp_sf)
plot(v)
plot(temp_sf)
